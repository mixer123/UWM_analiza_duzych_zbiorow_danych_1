{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46f3b902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8b4e027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/08 01:01:34 WARN Utils: Your hostname, mixer-N550JV resolves to a loopback address: 127.0.1.1; using 192.168.1.101 instead (on interface wlp4s0)\n",
      "23/04/08 01:01:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/08 01:01:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mixer/anaconda3/lib/python3.9/site-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc66790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlc.read.format('com.databricks.spark.csv').options(header = 'true',inferschema = 'true').load('iris_missing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9c248ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+----------+-------+\n",
      "|Sepallength|Sepalwidth|Petallength|Petalwidth|Species|\n",
      "+-----------+----------+-----------+----------+-------+\n",
      "|        5.1|       3.5|        1.4|       0.2| setosa|\n",
      "|        4.9|       3.0|        1.4|       0.2| setosa|\n",
      "|        4.7|       3.2|        1.3|       0.2| setosa|\n",
      "|        4.6|       3.1|        1.5|       0.2| setosa|\n",
      "|        5.0|       3.6|        1.4|       0.2| setosa|\n",
      "+-----------+----------+-----------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79caa231",
   "metadata": {},
   "source": [
    "## Exercise 38: Counting Missing Values in a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5251d5",
   "metadata": {},
   "source": [
    "In this exercise, we will learn how to count the missing values from the PySpark DataFrame column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72d6518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca7a592",
   "metadata": {},
   "source": [
    "2. Now, we will count the missing values in the Sepallength column of the Iris dataset loaded in the PySpark DataFrame df\n",
    "object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3aac0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(col('Sepallength').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25ff55bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df['Sepallength'].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86351d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(isnan('Sepallength')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c35d1b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(isnan(df['Sepallength'])).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a046b91a",
   "metadata": {},
   "source": [
    "when(), otherwise(), alias() functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "920fecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "c='Sepalwidth'\n",
    "#df.show()\n",
    "#when(col(c)>3.0,'BIG') #returns column\n",
    "#df.select(when(col(c)>3.0,'BIG')).show() #returns and shows dataframe\n",
    "#df.select(when(col(c)>3.0,'BIG').otherwise('SMALL')).show()\n",
    "#df.select(when((col(c)<4.0)&(col(c)>3.0),'BIG').when(col(c)>=4.0,'HUGE').otherwise('SMALL')).show()\n",
    "#df.select(when((col(c)<4.0)&(col(c)>3.0),'BIG').when(col(c)>=4.0,'HUGE').otherwise('SMALL').alias(c)).show()\n",
    "#df.select(count(when(col(c)>3.0,'BIG'))).show()\n",
    "#df.select(count(when(col(c)>3.0,'BIG').alias(c))).show()\n",
    "#for selecting you can ue Spark columns list:\n",
    "#col1 = when(col(c)>3.0,'BIG').otherwise(0)\n",
    "#col2 = when(col(c)<3.0,'SMALL').otherwise(0)\n",
    "#df.select([col1,col2]).show()\n",
    "#TRY to do Ex39 yourself as Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67c2d0c",
   "metadata": {},
   "source": [
    "## Exercise 39: Counting Missing Values in All DataFrame Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72e9334d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+----------+-------+\n",
      "|Sepallength|Sepalwidth|Petallength|Petalwidth|Species|\n",
      "+-----------+----------+-----------+----------+-------+\n",
      "|          5|         4|          3|         3|      0|\n",
      "+-----------+----------+-----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "df.select([count(when(isnan(c) | col(c).isNull(),c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8253812b",
   "metadata": {},
   "source": [
    "2. A simple way is to just use the describe() function, which gives the count of non-missing values for each column, along\n",
    "with a bunch of other summary statistics. Let's execute the following command in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be23282e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+------------------+------------------+---------+\n",
      "|summary|      Sepallength|         Sepalwidth|       Petallength|        Petalwidth|  Species|\n",
      "+-------+-----------------+-------------------+------------------+------------------+---------+\n",
      "|  count|              145|                146|               147|               147|      150|\n",
      "|   mean|             5.82| 3.0582191780821923|  3.73401360544218|1.1761904761904767|     null|\n",
      "| stddev|0.819349335346855|0.43672082920802746|1.7723329025952892|0.7521051126382675|     null|\n",
      "|    min|              4.3|                2.0|               1.0|               0.1|   setosa|\n",
      "|    max|              7.9|                4.4|               6.9|               2.5|virginica|\n",
      "+-------+-----------------+-------------------+------------------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.describe().show()\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9a60f0",
   "metadata": {},
   "source": [
    "## Fetching Missing Value Records from the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da8e248",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
