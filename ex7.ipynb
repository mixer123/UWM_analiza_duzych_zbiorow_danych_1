{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46f3b902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8b4e027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/26 16:14:46 WARN Utils: Your hostname, mixer-N550JV resolves to a loopback address: 127.0.1.1; using 192.168.237.224 instead (on interface wlp4s0)\n",
      "23/04/26 16:14:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/26 16:14:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mixer/anaconda3/lib/python3.9/site-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cd5111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc66790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlc.read.format('com.databricks.spark.csv').options(header = 'true',inferschema = 'true').load('iris_missing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9c248ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+----------+-------+\n",
      "|Sepallength|Sepalwidth|Petallength|Petalwidth|Species|\n",
      "+-----------+----------+-----------+----------+-------+\n",
      "|        5.1|       3.5|        1.4|       0.2| setosa|\n",
      "|        4.9|       3.0|        1.4|       0.2| setosa|\n",
      "|        4.7|       3.2|        1.3|       0.2| setosa|\n",
      "|        4.6|       3.1|        1.5|       0.2| setosa|\n",
      "|        5.0|       3.6|        1.4|       0.2| setosa|\n",
      "+-----------+----------+-----------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79caa231",
   "metadata": {},
   "source": [
    "## Exercise 38: Counting Missing Values in a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5251d5",
   "metadata": {},
   "source": [
    "In this exercise, we will learn how to count the missing values from the PySpark DataFrame column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72d6518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca7a592",
   "metadata": {},
   "source": [
    "2. Now, we will count the missing values in the Sepallength column of the Iris dataset loaded in the PySpark DataFrame df\n",
    "object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3aac0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(col('Sepallength').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25ff55bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df['Sepallength'].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86351d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(isnan('Sepallength')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c35d1b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(isnan(df['Sepallength'])).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a046b91a",
   "metadata": {},
   "source": [
    "when(), otherwise(), alias() functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "920fecc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+----------+-------+\n",
      "|Sepallength|Sepalwidth|Petallength|Petalwidth|Species|\n",
      "+-----------+----------+-----------+----------+-------+\n",
      "|        5.1|       3.5|        1.4|       0.2| setosa|\n",
      "|        4.9|       3.0|        1.4|       0.2| setosa|\n",
      "|        4.7|       3.2|        1.3|       0.2| setosa|\n",
      "|        4.6|       3.1|        1.5|       0.2| setosa|\n",
      "|        5.0|       3.6|        1.4|       0.2| setosa|\n",
      "|        5.4|       3.9|        1.7|       0.4| setosa|\n",
      "|        4.6|       3.4|        1.4|       0.3| setosa|\n",
      "|        5.0|       3.4|        1.5|       0.2| setosa|\n",
      "|        4.4|       2.9|        1.4|       0.2| setosa|\n",
      "|        4.9|       3.1|        1.5|       0.1| setosa|\n",
      "|        5.4|       3.7|        1.5|       0.2| setosa|\n",
      "|        4.8|       3.4|        1.6|       0.2| setosa|\n",
      "|        4.8|       3.0|        1.4|       0.1| setosa|\n",
      "|        4.3|       3.0|        1.1|       0.1| setosa|\n",
      "|        5.8|       4.0|        1.2|       0.2| setosa|\n",
      "|        5.7|       4.4|        1.5|       0.4| setosa|\n",
      "|        5.4|       3.9|        1.3|       0.4| setosa|\n",
      "|        5.1|       3.5|        1.4|       0.3| setosa|\n",
      "|        5.7|       3.8|        1.7|       0.3| setosa|\n",
      "|        5.1|       3.8|        1.5|       0.3| setosa|\n",
      "+-----------+----------+-----------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------------------------------+\n",
      "|Sepalwidthte ktore sa wieksze od 3|\n",
      "+----------------------------------+\n",
      "|                               BIG|\n",
      "|                              null|\n",
      "|                               BIG|\n",
      "|                               BIG|\n",
      "|                               BIG|\n",
      "|                               BIG|\n",
      "|                               BIG|\n",
      "|                               BIG|\n",
      "|                              null|\n",
      "|                               BIG|\n",
      "|                               BIG|\n",
      "|                               BIG|\n",
      "|                              null|\n",
      "|                              null|\n",
      "|                               BIG|\n",
      "|                               BIG|\n",
      "|                               BIG|\n",
      "|                               BIG|\n",
      "|                               BIG|\n",
      "|                               BIG|\n",
      "+----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c='Sepalwidth'\n",
    "df.show()\n",
    "# when(col(c)>3.0,'BIG')#   to zwraca kolumny i pisze BIG gdy >3.0\n",
    "df.select(when(col(c)>3.0,'BIG').alias(c+'te ktore sa wieksze od 3')).show() #returns and shows dataframe\n",
    "#df.select(when(col(c)>3.0,'BIG').otherwise('SMALL')).show()\n",
    "#df.select(when((col(c)<4.0)&(col(c)>3.0),'BIG').when(col(c)>=4.0,'HUGE').otherwise('SMALL')).show()\n",
    "#df.select(when((col(c)<4.0)&(col(c)>3.0),'BIG').when(col(c)>=4.0,'HUGE').otherwise('SMALL').alias(c)).show()\n",
    "#df.select(count(when(col(c)>3.0,'BIG'))).show()\n",
    "#df.select(count(when(col(c)>3.0,'BIG').alias(c))).show()\n",
    "#for selecting you can ue Spark columns list:\n",
    "# col1 = when(col(c)>3.0,'BIG').otherwise(0).alias('c>3.0,BIG')\n",
    "# col2 = when(col(c)<3.0,'SMALL').otherwise(0).alias('c<3')\n",
    "# df.select([col1,col2]).show()\n",
    "#TRY to do Ex39 yourself as Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67c2d0c",
   "metadata": {},
   "source": [
    "## Exercise 39: Counting Missing Values in All DataFrame Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72e9334d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+----------+-------+\n",
      "|Sepallength|Sepalwidth|Petallength|Petalwidth|Species|\n",
      "+-----------+----------+-----------+----------+-------+\n",
      "|          5|         4|          3|         3|      0|\n",
      "+-----------+----------+-----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "df.select([count(when(isnan(c) | col(c).isNull(),c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8253812b",
   "metadata": {},
   "source": [
    "2. A simple way is to just use the describe() function, which gives the count of non-missing values for each column, along\n",
    "with a bunch of other summary statistics. Let's execute the following command in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be23282e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+------------------+------------------+---------+\n",
      "|summary|      Sepallength|         Sepalwidth|       Petallength|        Petalwidth|  Species|\n",
      "+-------+-----------------+-------------------+------------------+------------------+---------+\n",
      "|  count|              145|                146|               147|               147|      150|\n",
      "|   mean|             5.82| 3.0582191780821923|  3.73401360544218|1.1761904761904767|     null|\n",
      "| stddev|0.819349335346855|0.43672082920802746|1.7723329025952892|0.7521051126382675|     null|\n",
      "|    min|              4.3|                2.0|               1.0|               0.1|   setosa|\n",
      "|    max|              7.9|                4.4|               6.9|               2.5|virginica|\n",
      "+-------+-----------------+-------------------+------------------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.describe().show()\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9a60f0",
   "metadata": {},
   "source": [
    "## Fetching Missing Value Records from the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab54d7fd",
   "metadata": {},
   "source": [
    "We can also filter out the records containing the missing value entries from the PySpark DataFrame using the following\n",
    "code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14e120c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+----------+----------+\n",
      "|Sepallength|Sepalwidth|Petallength|Petalwidth|   Species|\n",
      "+-----------+----------+-----------+----------+----------+\n",
      "|       null|       2.4|        3.7|       1.0|versicolor|\n",
      "|       null|       3.1|        4.7|       1.5|versicolor|\n",
      "|       null|      null|       null|       1.2|versicolor|\n",
      "|       null|       3.8|        6.7|      null| virginica|\n",
      "|       null|      null|       null|      null| virginica|\n",
      "+-----------+----------+-----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col('Sepallength').isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10e61e1",
   "metadata": {},
   "source": [
    "## Exercise 40: Removing Records with Missing Values from a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bf6dc8",
   "metadata": {},
   "source": [
    "In this exercise, we will remove the records containing missing value entries for the PySpark DataFrame. Let's perform the\n",
    "following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332f59d3",
   "metadata": {},
   "source": [
    "1. To remove the missing values from a particular column, use the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814e56ab",
   "metadata": {},
   "source": [
    "Usuwa Nan i zlicza ile jest po usunięciu !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2850c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('Sepallength').dropna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ac4a6c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "361249c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b514d5ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5c1098f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c84c67",
   "metadata": {},
   "source": [
    "### Exercise 41: Filling Missing Values with a Constant in a DataFrame Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18edd56",
   "metadata": {},
   "source": [
    "In this exercise, we will replace the missing value entries of the PySpark DataFrame column with a constant numeric\n",
    "value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757e825c",
   "metadata": {},
   "source": [
    "1. Let's replace the missing value entries in two columns with a constant numeric value of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d48f25e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "colNames = ['Sepallength','Sepalwidth']\n",
    "y = df.select(colNames).fillna(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87536796",
   "metadata": {},
   "source": [
    "Now, let's count the missing values in the new DataFrame, y, that we just created. The new DataFrame should have no\n",
    "missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21476f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|Sepallength|Sepalwidth|\n",
      "+-----------+----------+\n",
      "|          0|         0|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y.select([count(when(isnan(i) | col(i).isNull(), i)).alias(i) for i in y.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432fc68f",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ca1b569",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlc.read.format('com.databricks.spark.csv').options(header = 'true',inferschema = 'true').load('iris_missing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc0ea63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.150594543846508"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr('Sepallength', 'Sepalwidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ca82e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
