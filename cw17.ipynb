{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db8ec521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0429f060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/19 11:13:36 WARN Utils: Your hostname, mixer-desktop resolves to a loopback address: 127.0.1.1; using 192.168.1.6 instead (on interface enp4s0)\n",
      "23/03/19 11:13:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/19 11:13:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"python Spark Session\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20d2a7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('people.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4a80ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+\n",
      "|  name|age|height|\n",
      "+------+---+------+\n",
      "|  Adam| 23|  1.98|\n",
      "| Fiona| 34|  1.67|\n",
      "|  Mary| 26|  1.72|\n",
      "|George| 21|  1.83|\n",
      "+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce460fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'age'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1561c320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(df['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daa51c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "| 23|\n",
      "| 34|\n",
      "| 26|\n",
      "| 21|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f5f8ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|  name|\n",
      "+---+------+\n",
      "| 23|  Adam|\n",
      "| 34| Fiona|\n",
      "| 26|  Mary|\n",
      "| 21|George|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['age'],df['name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8309767",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/home/mixer/analiza_duzych_zbiorow_danych/result.csv already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5553/2714305184.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'result.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1238\u001b[0m             \u001b[0mlineSep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlineSep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m         )\n\u001b[0;32m-> 1240\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m     def orc(\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path file:/home/mixer/analiza_duzych_zbiorow_danych/result.csv already exists."
     ]
    }
   ],
   "source": [
    "df.write.csv('result.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec80ace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.json('result.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f0d2eb",
   "metadata": {},
   "source": [
    "Activity: Data Conversion\n",
    "Convert data from cars.data file, used on previous lesson, from CSV format to spark JSON format.\n",
    "You can use pandas in this process. Notice differences between pandas JSON and spark JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f9211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cars = spark.read.csv('cars.data', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db6d46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cars.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236f96f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cars.columns = ['mpg', 'cylinders', 'displacement [cu]','horsepower','weight', 'acceleration', 'year', 'origin', 'name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a084d97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cars_json = df_cars.toJSON()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f67361",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cars_json.write.json('cars.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febf1d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd = spark.read.text('poem.txt').rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9131db57",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = df_rdd.map(lambda line: line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f4ec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8ff7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = lines.flatMap(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197ee083",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "splits.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867ba711",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_splits = splits.map(lambda x: x.lower().strip().strip('.').strip(' ,').strip('-').strip(':'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7011d4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_splits.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9783070d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = ['the', 'a', ',',';','.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfa5a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = lower_splits.filter(lambda x: x not in prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bbd32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff89865",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = tokens.map(lambda x: [x,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30226678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c3b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "countwords = token_list.reduceByKey(add).sortBy(lambda x: x[1], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65e6dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "countwords.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4a9369",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
